{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Gold Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying sys.path to include '/workspace/etl' and '/workspace/etl/utils' in the list of paths\n",
    "import sys\n",
    "sys.path.append('/workspace/etl')\n",
    "sys.path.append('/workspace/etl/utils')\n",
    "print(sys.path)\n",
    "\n",
    "# Importing Modules\n",
    "import os\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from extract import DataExtractor\n",
    "from step2_load_to_postgres import DataLoader\n",
    "from utils_connection import get_s3_parquet_file_key, get_connection_uri\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Environment Variables\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch AWS credentials from environment variables\n",
    "s3_access_key_id = os.getenv('S3_ACCESS_KEY_ID')\n",
    "s3_secret_access_key = os.getenv('S3_SECRET_ACCESS_KEY')\n",
    "s3_region = os.getenv('S3_REGION')\n",
    "s3_bucket_name = os.getenv('S3_BUCKET_NAME')\n",
    "\n",
    "print(\"S3_ACCESS_KEY_ID: \", s3_access_key_id)\n",
    "print(\"S3_SECRET_ACCESS_KEY: \", s3_secret_access_key)\n",
    "print(\"S3_BUCKET_NAME: \", s3_bucket_name)\n",
    "\n",
    "# Initialize a session using boto3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=s3_access_key_id,\n",
    "    aws_secret_access_key=s3_secret_access_key,\n",
    "    region_name=s3_region\n",
    ")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "# Example: List objects in the bucket to verify access\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket_name  # Ensure bucket_name is converted to string\n",
    "    )\n",
    "    # Print object keys if listing was successful\n",
    "    print(\"Objects in bucket:\")\n",
    "    for obj in response.get('Contents', []):\n",
    "        print(obj['Key'])\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing bucket: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGoldEDA:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the DataGoldEDA class.\"\"\"\n",
    "        self.engine = create_engine(get_connection_uri())\n",
    "\n",
    "    def get_data_from_postgres_to_pd(self, schema_name: str, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Loads data from a PostgreSQL table in a given schema into a Pandas DataFrame.\"\"\"\n",
    "        query = f\"SELECT * FROM {schema_name}.{table_name}\"\n",
    "        try:\n",
    "            df = pd.read_sql(query, self.engine)\n",
    "            print(f\"Data loaded successfully from {schema_name}.{table_name}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {schema_name}.{table_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Add the methods for the metrics here\n",
    "    def matching_email_only(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the count of rows where there's a match in email_hash but not in phone_hash.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT COUNT(*) AS email_match_only\n",
    "        FROM gold.lead_quality_matching\n",
    "        WHERE email_match = TRUE AND phone_match = FALSE;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def matching_phone_only(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the count of rows where there's a match in phone_hash but not in email_hash.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT COUNT(*) AS phone_match_only\n",
    "        FROM gold.lead_quality_matching\n",
    "        WHERE phone_match = TRUE AND email_match = FALSE;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def matching_both(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the count of rows where there's a match in both email_hash and phone_hash.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT COUNT(*) AS both_match\n",
    "        FROM gold.lead_quality_matching\n",
    "        WHERE email_match = TRUE AND phone_match = TRUE;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "    \n",
    "    def matching_all(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the count of rows where lead_uuid is NULL.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT COUNT(*) as null_leads\n",
    "        FROM gold.lead_quality_matching\n",
    "        WHERE lead_uuid is null;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def total_leads_per_partition(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the total number of leads per partition date.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT _partition_date, COUNT(DISTINCT lead_uuid) AS total_leads\n",
    "        FROM gold.lead_quality_matching\n",
    "        GROUP BY _partition_date\n",
    "        ORDER BY _partition_date;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def get_total_and_new_leads_per_partition(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates the total and new leads for each _partition_date.\n",
    "        A new lead is one that has not appeared in any of the previous _partition_dates.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        WITH sorted_leads AS (\n",
    "            SELECT \n",
    "                _partition_date, \n",
    "                lead_uuid, \n",
    "                ROW_NUMBER() OVER (PARTITION BY lead_uuid ORDER BY _partition_date) AS row_num\n",
    "            FROM gold.lead_quality_matching\n",
    "        ),\n",
    "        new_leads AS (\n",
    "            SELECT \n",
    "                _partition_date, \n",
    "                COUNT(DISTINCT lead_uuid) AS new_leads\n",
    "            FROM sorted_leads\n",
    "            WHERE row_num = 1\n",
    "            GROUP BY _partition_date\n",
    "        ),\n",
    "        total_leads AS (\n",
    "            SELECT \n",
    "                _partition_date, \n",
    "                COUNT(DISTINCT lead_uuid) AS total_leads\n",
    "            FROM gold.lead_quality_matching\n",
    "            GROUP BY _partition_date\n",
    "        )\n",
    "        SELECT \n",
    "            t._partition_date, \n",
    "            t.total_leads, \n",
    "            n.new_leads\n",
    "        FROM total_leads t\n",
    "        LEFT JOIN new_leads n \n",
    "        ON t._partition_date = n._partition_date\n",
    "        ORDER BY t._partition_date;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_sql(query, self.engine)\n",
    "            print(\"Total and new leads per partition date calculated successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error running query: {e}\")\n",
    "            return None\n",
    "\n",
    "    def overall_lead_quality_distribution(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the distribution of lead quality flags.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT lead_quality_flag, COUNT(DISTINCT lead_uuid) AS count\n",
    "        FROM gold.lead_quality_matching\n",
    "        GROUP BY lead_quality_flag;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "    \n",
    "    def lead_quality_distribution_by_partition(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the distribution of lead quality flags per _partition_date.\n",
    "\n",
    "        This function retrieves the count of unique leads for each quality classification \n",
    "        (High Quality, Medium Quality, Low Quality) across all available partition dates. \n",
    "\n",
    "        Key Points:\n",
    "        - Lead Quality Flags: The function categorizes leads based on their quality flag, which\n",
    "        is derived from specific criteria, such as the values of `set` and `demo`.\n",
    "        - Daily Distribution: The counts are aggregated by the `_partition_date`, allowing for \n",
    "        analysis of lead quality over time. This provides insight into how lead quality changes \n",
    "        across different days.\n",
    "        - Data Overview: By grouping the results by both `_partition_date` and `lead_quality_flag`,\n",
    "        the function offers a clear view of how many leads fall into each quality category \n",
    "        for each day in the dataset.\n",
    "        \n",
    "        Considerations:\n",
    "        - New vs. Existing Leads: This function provides an overview of lead quality distribution \n",
    "        but does not distinguish between leads that are new on a given day versus those that were \n",
    "        present on previous days.\n",
    "        - Temporal Trends: The data retrieved allows for temporal analysis, helping to identify \n",
    "        patterns in lead quality over time, which can be essential for assessing the effectiveness \n",
    "        of lead generation strategies.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the date, lead quality flag, and the count of \n",
    "            unique leads in each category for that date.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            _partition_date,\n",
    "            lead_quality_flag, \n",
    "            COUNT(DISTINCT lead_uuid) AS count\n",
    "        FROM gold.lead_quality_matching\n",
    "        GROUP BY _partition_date, lead_quality_flag\n",
    "        ORDER BY _partition_date, lead_quality_flag;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "\n",
    "    def new_high_quality_leads_daily_count(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the daily count of new high-quality leads.\n",
    "\n",
    "        This function counts the number of unique leads classified as 'High Quality'\n",
    "        that appear for the first time on each day. It provides insights into the \n",
    "        growth of new high-quality leads over time.\n",
    "\n",
    "        Key Points:\n",
    "        - High Quality Definition: The function relies on the classification of high-quality leads,\n",
    "        based on specific criteria (e.g., leads with `set = 1` and `demo = 1`).\n",
    "        - New Lead Identification: It counts only those leads that qualify as high quality\n",
    "        for the first time on each date, ensuring accurate tracking of lead growth.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the date and the count of new high-quality leads.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        WITH high_quality_leads AS (\n",
    "            SELECT \n",
    "                lead_uuid, \n",
    "                _partition_date,\n",
    "                ROW_NUMBER() OVER (PARTITION BY lead_uuid ORDER BY _partition_date) AS first_high_quality_day\n",
    "            FROM gold.lead_quality_matching\n",
    "            WHERE lead_quality_flag = 'High Quality'\n",
    "        ),\n",
    "        daily_new_high_quality_counts AS (\n",
    "            SELECT \n",
    "                _partition_date, \n",
    "                COUNT(lead_uuid) AS new_high_quality_count\n",
    "            FROM high_quality_leads\n",
    "            WHERE first_high_quality_day = 1  -- Only count leads that are new high-quality for the first time\n",
    "            GROUP BY _partition_date\n",
    "        )\n",
    "        SELECT \n",
    "            _partition_date, \n",
    "            new_high_quality_count\n",
    "        FROM daily_new_high_quality_counts\n",
    "        ORDER BY _partition_date;\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, self.engine)\n",
    "    \n",
    "    def plot_new_high_quality_leads_trend(self):\n",
    "        \"\"\"\n",
    "        Plot the trend of new high-quality leads over time.\n",
    "\n",
    "        This function retrieves the daily counts of new high-quality leads\n",
    "        and creates a line graph to visualize the trend over the specified period,\n",
    "        excluding the first day's data to avoid the peak.\n",
    "        \"\"\"\n",
    "        # Retrieve the daily counts of new high-quality leads\n",
    "        new_high_quality_leads = self.new_high_quality_leads_daily_count()\n",
    "\n",
    "        # Check if data is available\n",
    "        if new_high_quality_leads.empty:\n",
    "            print(\"No data available to plot.\")\n",
    "            return\n",
    "\n",
    "        # Convert _partition_date to datetime if necessary\n",
    "        new_high_quality_leads['_partition_date'] = pd.to_datetime(new_high_quality_leads['_partition_date'])\n",
    "\n",
    "        # Exclude the first day to avoid the peak\n",
    "        new_high_quality_leads = new_high_quality_leads.iloc[1:]\n",
    "\n",
    "        # Plotting the trend\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(new_high_quality_leads['_partition_date'], new_high_quality_leads['new_high_quality_count'], marker='o')\n",
    "        \n",
    "        # Adding title and labels\n",
    "        plt.title('Trend of New High-Quality Leads Over Time (Excluding First Day)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Number of New High-Quality Leads')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Example schema and table names\n",
    "schema_names = ['gold']\n",
    "gold_table_names = ['lead_quality_matching']\n",
    "\n",
    "# Instantiate the DataGoldEDA\n",
    "eda = DataGoldEDA()\n",
    "\n",
    "# Get Gold Schema\n",
    "gold_schema = schema_names[0]\n",
    "\n",
    "# Load data from the gold table and perform EDA\n",
    "for table_name in gold_table_names:\n",
    "    gold_data = eda.get_data_from_postgres_to_pd(gold_schema, table_name)\n",
    "    if gold_data is not None:\n",
    "        logging.info(f\"\\nPerforming EDA on table: {table_name}\")\n",
    "\n",
    "# Perform various analyses using the defined methods\n",
    "try:\n",
    "    # Get counts for various matching scenarios\n",
    "    email_match_only = eda.matching_email_only()\n",
    "    logging.info(f\"Email Match Only: {email_match_only.iloc[0]['email_match_only']}\")\n",
    "\n",
    "    phone_match_only = eda.matching_phone_only()\n",
    "    logging.info(f\"Phone Match Only: {phone_match_only.iloc[0]['phone_match_only']}\")\n",
    "\n",
    "    both_match = eda.matching_both()\n",
    "    logging.info(f\"Both Match: {both_match.iloc[0]['both_match']}\")\n",
    "\n",
    "    null_leads = eda.matching_all()\n",
    "    logging.info(f\"Null Leads: {null_leads.iloc[0]['null_leads']}\")\n",
    "\n",
    "    # Get total leads per partition\n",
    "    total_leads_per_partition = eda.total_leads_per_partition()\n",
    "    logging.info(f\"Total Leads Per Partition:\\n{total_leads_per_partition}\")\n",
    "\n",
    "    # Get total and new leads per partition\n",
    "    total_and_new_leads = eda.get_total_and_new_leads_per_partition()\n",
    "    logging.info(f\"Total and New Leads Per Partition:\\n{total_and_new_leads}\")\n",
    "\n",
    "    # Get overall lead quality distribution\n",
    "    lead_quality_distribution = eda.overall_lead_quality_distribution()\n",
    "    logging.info(f\"Overall Lead Quality Distribution:\\n{lead_quality_distribution}\")\n",
    "\n",
    "    # Get lead quality distribution by partition\n",
    "    lead_quality_distribution_by_partition = eda.lead_quality_distribution_by_partition()\n",
    "    logging.info(f\"Lead Quality Distribution by Partition:\\n{lead_quality_distribution_by_partition}\")\n",
    "\n",
    "    # Get daily increase of high-quality leads\n",
    "    new_daily_high_quality_increase = eda.new_high_quality_leads_daily_count()\n",
    "    logging.info(f\"Daily High Quality Leads Increase:\\n{new_daily_high_quality_increase}\")\n",
    "\n",
    "    # Plot the trend of new high-quality leads\n",
    "    eda.plot_new_high_quality_leads_trend()\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during EDA: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
