{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying sys.path to include '/workspace/etl' and '/workspace/etl/utils' in the list of paths\n",
    "import sys\n",
    "sys.path.append('/workspace/etl')\n",
    "sys.path.append('/workspace/etl/utils')\n",
    "print(sys.path)\n",
    "\n",
    "# Importing Modules\n",
    "import os\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from extract import DataExtractor\n",
    "from step2_load_to_postgres import DataLoader\n",
    "from utils_connection import get_s3_parquet_file_key, get_connection_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Environment Variables\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch AWS credentials from environment variables\n",
    "s3_access_key_id = os.getenv('S3_ACCESS_KEY_ID')\n",
    "s3_secret_access_key = os.getenv('S3_SECRET_ACCESS_KEY')\n",
    "s3_region = os.getenv('S3_REGION')\n",
    "s3_bucket_name = os.getenv('S3_BUCKET_NAME')\n",
    "\n",
    "print(\"S3_ACCESS_KEY_ID: \", s3_access_key_id)\n",
    "print(\"S3_SECRET_ACCESS_KEY: \", s3_secret_access_key)\n",
    "print(\"S3_BUCKET_NAME: \", s3_bucket_name)\n",
    "\n",
    "# Initialize a session using boto3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=s3_access_key_id,\n",
    "    aws_secret_access_key=s3_secret_access_key,\n",
    "    region_name=s3_region\n",
    ")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "# Example: List objects in the bucket to verify access\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket_name  # Ensure bucket_name is converted to string\n",
    "    )\n",
    "    # Print object keys if listing was successful\n",
    "    print(\"Objects in bucket:\")\n",
    "    for obj in response.get('Contents', []):\n",
    "        print(obj['Key'])\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing bucket: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSilverEDA:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the DataTransform class.\"\"\"\n",
    "        self.engine = create_engine(get_connection_uri())\n",
    "\n",
    "    def get_data_from_postgres_to_pd(self, schema_name: str, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Loads data from a PostgreSQL table in a given schema into a Pandas DataFrame.\"\"\"\n",
    "        query = f\"SELECT * FROM {schema_name}.{table_name}\"\n",
    "        try:\n",
    "            df = pd.read_sql(query, self.engine)\n",
    "            print(f\"Data loaded successfully from {schema_name}.{table_name}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {schema_name}.{table_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def display_csv_summary_statistics(self, df: pd.DataFrame):\n",
    "        \"\"\"Displays summary statistics for the cleaned DataFrame.\"\"\"\n",
    "        print(\"Summary statistics:\")\n",
    "        print(df[['entry_date', 'appt_date', 'state', 'zip', 'set', 'demo']].head())\n",
    "        \n",
    "        # Count of null values in key columns\n",
    "        print(\"\\nCount of null values in key columns:\")\n",
    "        print(df[['entry_date', 'appt_date', 'state', 'zip', 'set', 'demo']].isnull().sum())\n",
    "        \n",
    "        # Checking specific columns for unique values\n",
    "        columns_to_check = [\n",
    "            \"_extraction_date\", \n",
    "            \"_partition_date\", \n",
    "            \"state\", \n",
    "            \"zip\", \n",
    "            \"set\", \n",
    "            \"demo\", \n",
    "            \"job_status\"\n",
    "        ]\n",
    "        \n",
    "        for column in columns_to_check:\n",
    "            print(f\"\\nUnique values in '{column}' column:\")\n",
    "            print(df[column].unique())\n",
    "        \n",
    "        # Checking the data types of all columns\n",
    "        print(\"\\nData types of all columns:\")\n",
    "        print(df.dtypes)\n",
    "\n",
    "        # Basic information about the cleaned DataFrame\n",
    "        print(\"\\nBasic information about the cleaned DataFrame:\")\n",
    "        print(df.info())\n",
    "\n",
    "\n",
    "    def display_parquet_data_info(self, df: pd.DataFrame):\n",
    "        \"\"\"Displays information related to Parquet data.\"\"\"\n",
    "        # Check the shape of the DataFrame\n",
    "        print(f\"DataFrame Shape: {df.shape}\")\n",
    "\n",
    "        # Check for missing values\n",
    "        missing_values = df.isnull().sum()\n",
    "        print(\"Missing Values in Each Column:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "\n",
    "        # Check for duplicates\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "        print(f\"Number of Duplicate Rows: {duplicate_count}\")\n",
    "\n",
    "        # Check for duplicates in the combination of email_hash and phone_hash\n",
    "        email_phone_duplicates_count = df.duplicated(subset=['email_hash', 'phone_hash']).sum()\n",
    "        print(f\"Number of Duplicate Rows based on email_hash and phone_hash: {email_phone_duplicates_count}\")\n",
    "\n",
    "        # Check the uniqueness of the first three ID columns\n",
    "        for col in df.columns[:3]:  # Assuming first three columns are IDs\n",
    "            unique_count = df[col].nunique()\n",
    "            total_count = df[col].shape[0]\n",
    "            print(f\"Unique values in '{col}': {unique_count} out of {total_count} total\")\n",
    "\n",
    "        # Check data types of the columns\n",
    "        print(\"\\nData Types of Columns:\")\n",
    "        print(df.dtypes)\n",
    "\n",
    "        # Check the first few rows of the DataFrame\n",
    "        print(\"\\nFirst Few Rows of Data:\")\n",
    "        print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Example schema and table names\n",
    "schema_names = ['bronze', 'silver']\n",
    "bronze_table_names = ['leads_parquet', 'csv_snapshots']\n",
    "silver_table_names = ['stg_leads_parquet', 'stg_csv_snapshots']\n",
    "\n",
    "# Instantiate the DataSilverEDA\n",
    "eda = DataSilverEDA()\n",
    "\n",
    "# Get Silver Schema\n",
    "silver_schema = schema_names[1]\n",
    "\n",
    "# Load data from the silver tables and perform EDA\n",
    "for table_name in silver_table_names:\n",
    "    silver_data = eda.get_data_from_postgres_to_pd('silver', table_name)\n",
    "    if silver_data is not None:\n",
    "        print(f\"\\nPerforming EDA on table: {table_name}\")\n",
    "        \n",
    "        # Determine the type of table and perform EDA accordingly\n",
    "        if \"stg_leads_parquet\" in table_name:\n",
    "            eda.display_parquet_data_info(silver_data)  # For Parquet files\n",
    "        elif \"stg_csv_snapshots\" in table_name:\n",
    "            eda.display_csv_summary_statistics(silver_data)  # For CSV files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
